{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "2c90498d",
   "metadata": {
    "papermill": {
     "duration": 0.005124,
     "end_time": "2025-09-18T14:29:18.490737",
     "exception": false,
     "start_time": "2025-09-18T14:29:18.485613",
     "status": "completed"
    },
    "tags": []
   },
   "source": [
    "## Imports"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "169519c8",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-09-18T14:29:18.500937Z",
     "iopub.status.busy": "2025-09-18T14:29:18.500643Z",
     "iopub.status.idle": "2025-09-18T14:29:23.231423Z",
     "shell.execute_reply": "2025-09-18T14:29:23.230453Z"
    },
    "papermill": {
     "duration": 4.737741,
     "end_time": "2025-09-18T14:29:23.233191",
     "exception": false,
     "start_time": "2025-09-18T14:29:18.495450",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "import os\n",
    "from pathlib import Path\n",
    "import datetime\n",
    "from dataclasses import dataclass, asdict\n",
    "\n",
    "import polars as pl \n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "from sklearn.linear_model import ElasticNet, ElasticNetCV, LinearRegression\n",
    "from sklearn.preprocessing import StandardScaler\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f775b419",
   "metadata": {
    "papermill": {
     "duration": 0.003635,
     "end_time": "2025-09-18T14:29:23.241200",
     "exception": false,
     "start_time": "2025-09-18T14:29:23.237565",
     "status": "completed"
    },
    "tags": []
   },
   "source": [
    "## Project Directory Structure"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "e6bac6fe",
   "metadata": {
    "_cell_guid": "b1076dfc-b9ad-4769-8c92-a6c4dae69d19",
    "_uuid": "8f2839f25d086af736a60e9eeb907d3b93b6e0e5",
    "execution": {
     "iopub.execute_input": "2025-09-18T14:29:23.250751Z",
     "iopub.status.busy": "2025-09-18T14:29:23.250273Z",
     "iopub.status.idle": "2025-09-18T14:29:23.260080Z",
     "shell.execute_reply": "2025-09-18T14:29:23.259061Z"
    },
    "papermill": {
     "duration": 0.016302,
     "end_time": "2025-09-18T14:29:23.261493",
     "exception": false,
     "start_time": "2025-09-18T14:29:23.245191",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "# This Python 3 environment comes with many helpful analytics libraries installed\n",
    "# It is defined by the kaggle/python Docker image: https://github.com/kaggle/docker-python\n",
    "# For example, here's several helpful packages to load\n",
    "\n",
    "# Input data files are available in the read-only \"../input/\" directory\n",
    "# For example, running this (by clicking run or pressing Shift+Enter) will list all files under the input directory\n",
    "\n",
    "import os\n",
    "for dirname, _, filenames in os.walk('/kaggle/input'):\n",
    "    for filename in filenames:\n",
    "        print(os.path.join(dirname, filename))\n",
    "\n",
    "# You can write up to 20GB to the current directory (/kaggle/working/) that gets preserved as output when you create a version using \"Save & Run All\" \n",
    "# You can also write temporary files to /kaggle/temp/, but they won't be saved outside of the current session"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a2701ad4",
   "metadata": {
    "papermill": {
     "duration": 0.003833,
     "end_time": "2025-09-18T14:29:23.269584",
     "exception": false,
     "start_time": "2025-09-18T14:29:23.265751",
     "status": "completed"
    },
    "tags": []
   },
   "source": [
    "## Configurations"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "07fd5bf0",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-09-18T14:29:23.278787Z",
     "iopub.status.busy": "2025-09-18T14:29:23.278442Z",
     "iopub.status.idle": "2025-09-18T14:29:23.283902Z",
     "shell.execute_reply": "2025-09-18T14:29:23.283084Z"
    },
    "papermill": {
     "duration": 0.012051,
     "end_time": "2025-09-18T14:29:23.285590",
     "exception": false,
     "start_time": "2025-09-18T14:29:23.273539",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "# ============ PATHS ============\n",
    "DATA_PATH: Path = Path(r'\\Users\\chenx\\Documents\\Hull-comp\\hull-tactical-market-prediction')\n",
    "\n",
    "# ============ RETURNS TO SIGNAL CONFIGS ============\n",
    "MIN_SIGNAL: float = 0.0                         # Minimum value for the daily signal \n",
    "MAX_SIGNAL: float = 2.0                         # Maximum value for the daily signal \n",
    "SIGNAL_MULTIPLIER: float = 400.0                # Multiplier of the OLS market forward excess returns predictions to signal \n",
    "\n",
    "# ============ MODEL CONFIGS ============\n",
    "CV: int = 10                                    # Number of cross validation folds in the model fitting\n",
    "L1_RATIO: float = 0.5                           # ElasticNet mixing parameter\n",
    "ALPHAS: np.ndarray = np.logspace(-4, 2, 100)    # Constant that multiplies the penalty terms\n",
    "MAX_ITER: int = 1000000                         # The maximum number of iterations"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f17c31de",
   "metadata": {
    "papermill": {
     "duration": 0.003858,
     "end_time": "2025-09-18T14:29:23.344697",
     "exception": false,
     "start_time": "2025-09-18T14:29:23.340839",
     "status": "completed"
    },
    "tags": []
   },
   "source": [
    "## Dataset Loading/Creating Helper Functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "f60aa6ef",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-09-18T14:29:23.355071Z",
     "iopub.status.busy": "2025-09-18T14:29:23.354437Z",
     "iopub.status.idle": "2025-09-18T14:29:23.367560Z",
     "shell.execute_reply": "2025-09-18T14:29:23.366772Z"
    },
    "papermill": {
     "duration": 0.020226,
     "end_time": "2025-09-18T14:29:23.368947",
     "exception": false,
     "start_time": "2025-09-18T14:29:23.348721",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "def load_trainset() -> pl.DataFrame:\n",
    "    \"\"\"\n",
    "    Loads and preprocesses the training dataset.\n",
    "\n",
    "    Returns:\n",
    "        pl.DataFrame: The preprocessed training DataFrame.\n",
    "    \"\"\"\n",
    "    return (\n",
    "        pl.read_csv(DATA_PATH / \"train.csv\")\n",
    "        .rename({'market_forward_excess_returns':'target'})\n",
    "        .with_columns(\n",
    "            pl.exclude('date_id').cast(pl.Float64, strict=False)\n",
    "        )\n",
    "        .head(-10)\n",
    "    )\n",
    "\n",
    "def load_testset() -> pl.DataFrame:\n",
    "    \"\"\"\n",
    "    Loads and preprocesses the testing dataset.\n",
    "\n",
    "    Returns:\n",
    "        pl.DataFrame: The preprocessed testing DataFrame.\n",
    "    \"\"\"\n",
    "    return (\n",
    "        pl.read_csv(DATA_PATH / \"test.csv\")\n",
    "        .rename({'lagged_forward_returns':'target'})\n",
    "        .with_columns(\n",
    "            pl.exclude('date_id').cast(pl.Float64, strict=False)\n",
    "        )\n",
    "    )\n",
    "\n",
    "    \n",
    "def join_train_test_dataframes(train: pl.DataFrame, test: pl.DataFrame) -> pl.DataFrame:\n",
    "    \"\"\"\n",
    "    Joins two dataframes by common columns and concatenates them vertically.\n",
    "\n",
    "    Args:\n",
    "        train (pl.DataFrame): The training DataFrame.\n",
    "        test (pl.DataFrame): The testing DataFrame.\n",
    "\n",
    "    Returns:\n",
    "        pl.DataFrame: A single DataFrame with vertically stacked data from common columns.\n",
    "    \"\"\"\n",
    "    common_columns: list[str] = [col for col in train.columns if col in test.columns]\n",
    "    \n",
    "    return pl.concat([train.select(common_columns), test.select(common_columns)], how=\"vertical\")\n",
    "\n",
    "def split_dataset(train: pl.DataFrame, test: pl.DataFrame, features: list[str]) -> DatasetOutput: \n",
    "    \"\"\"\n",
    "    Splits the data into features (X) and target (y), and scales the features.\n",
    "\n",
    "    Args:\n",
    "        train (pl.DataFrame): The processed training DataFrame.\n",
    "        test (pl.DataFrame): The processed testing DataFrame.\n",
    "        features (list[str]): List of features to used in model. \n",
    "\n",
    "    Returns:\n",
    "        DatasetOutput: A dataclass containing the scaled feature sets, target series, and the fitted scaler.\n",
    "    \"\"\"\n",
    "    X_train = train.drop(['date_id','target']) \n",
    "    y_train = train.get_column('target')\n",
    "    X_test = test.drop(['date_id','target']) \n",
    "    y_test = test.get_column('target')\n",
    "    \n",
    "    scaler = StandardScaler() \n",
    "    \n",
    "    X_train_scaled_np = scaler.fit_transform(X_train)\n",
    "    X_train = pl.from_numpy(X_train_scaled_np, schema=features)\n",
    "    \n",
    "    X_test_scaled_np = scaler.transform(X_test)\n",
    "    X_test = pl.from_numpy(X_test_scaled_np, schema=features)\n",
    "    \n",
    "    \n",
    "    return DatasetOutput(\n",
    "        X_train = X_train,\n",
    "        y_train = y_train, \n",
    "        X_test = X_test, \n",
    "        y_test = y_test,\n",
    "        scaler = scaler\n",
    "    )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "b95b8cb6",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-09-18T14:29:23.409029Z",
     "iopub.status.busy": "2025-09-18T14:29:23.408103Z",
     "iopub.status.idle": "2025-09-18T14:29:23.902287Z",
     "shell.execute_reply": "2025-09-18T14:29:23.901564Z"
    },
    "papermill": {
     "duration": 0.500419,
     "end_time": "2025-09-18T14:29:23.903843",
     "exception": false,
     "start_time": "2025-09-18T14:29:23.403424",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['date_id', 'D1', 'D2', 'D3', 'D4', 'D5', 'D6', 'D7', 'D8', 'D9', 'E1', 'E10', 'E11', 'E12', 'E13', 'E14', 'E15', 'E16', 'E17', 'E18', 'E19', 'E2', 'E20', 'E3', 'E4', 'E5', 'E6', 'E7', 'E8', 'E9', 'I1', 'I2', 'I3', 'I4', 'I5', 'I6', 'I7', 'I8', 'I9', 'M1', 'M10', 'M11', 'M12', 'M13', 'M14', 'M15', 'M16', 'M17', 'M18', 'M2', 'M3', 'M4', 'M5', 'M6', 'M7', 'M8', 'M9', 'P1', 'P10', 'P11', 'P12', 'P13', 'P2', 'P3', 'P4', 'P5', 'P6', 'P7', 'P8', 'P9', 'S1', 'S10', 'S11', 'S12', 'S2', 'S3', 'S4', 'S5', 'S6', 'S7', 'S8', 'S9', 'V1', 'V10', 'V11', 'V12', 'V13', 'V2', 'V3', 'V4', 'V5', 'V6', 'V7', 'V8', 'V9', 'forward_returns', 'risk_free_rate', 'target'] shape: (9_011, 98)\n",
      "┌─────────┬─────┬─────┬─────┬───┬───────────┬─────────────────┬────────────────┬───────────┐\n",
      "│ date_id ┆ D1  ┆ D2  ┆ D3  ┆ … ┆ V9        ┆ forward_returns ┆ risk_free_rate ┆ target    │\n",
      "│ ---     ┆ --- ┆ --- ┆ --- ┆   ┆ ---       ┆ ---             ┆ ---            ┆ ---       │\n",
      "│ i64     ┆ f64 ┆ f64 ┆ f64 ┆   ┆ f64       ┆ f64             ┆ f64            ┆ f64       │\n",
      "╞═════════╪═════╪═════╪═════╪═══╪═══════════╪═════════════════╪════════════════╪═══════════╡\n",
      "│ 0       ┆ 0.0 ┆ 0.0 ┆ 0.0 ┆ … ┆ null      ┆ -0.002421       ┆ 0.000301       ┆ -0.003038 │\n",
      "│ 1       ┆ 0.0 ┆ 0.0 ┆ 0.0 ┆ … ┆ null      ┆ -0.008495       ┆ 0.000303       ┆ -0.009114 │\n",
      "│ 2       ┆ 0.0 ┆ 0.0 ┆ 0.0 ┆ … ┆ null      ┆ -0.009624       ┆ 0.000301       ┆ -0.010243 │\n",
      "│ 3       ┆ 0.0 ┆ 0.0 ┆ 0.0 ┆ … ┆ null      ┆ 0.004662        ┆ 0.000299       ┆ 0.004046  │\n",
      "│ 4       ┆ 0.0 ┆ 0.0 ┆ 0.0 ┆ … ┆ null      ┆ -0.011686       ┆ 0.000299       ┆ -0.012301 │\n",
      "│ …       ┆ …   ┆ …   ┆ …   ┆ … ┆ …         ┆ …               ┆ …              ┆ …         │\n",
      "│ 9006    ┆ 0.0 ┆ 0.0 ┆ 0.0 ┆ … ┆ -0.550673 ┆ -0.003708       ┆ 0.000153       ┆ -0.004173 │\n",
      "│ 9007    ┆ 0.0 ┆ 0.0 ┆ 0.0 ┆ … ┆ -0.465105 ┆ 0.005963        ┆ 0.0001525      ┆ 0.005499  │\n",
      "│ 9008    ┆ 0.0 ┆ 0.0 ┆ 0.0 ┆ … ┆ -0.530228 ┆ -0.002897       ┆ 0.0001525      ┆ -0.003362 │\n",
      "│ 9009    ┆ 0.0 ┆ 0.0 ┆ 0.0 ┆ … ┆ -0.512769 ┆ -0.027028       ┆ 0.000153       ┆ -0.027493 │\n",
      "│ 9010    ┆ 0.0 ┆ 0.0 ┆ 0.0 ┆ … ┆ -0.015503 ┆ 0.015344        ┆ 0.000153       ┆ 0.014879  │\n",
      "└─────────┴─────┴─────┴─────┴───┴───────────┴─────────────────┴────────────────┴───────────┘\n",
      "['date_id', 'D1', 'D2', 'D3', 'D4', 'D5', 'D6', 'D7', 'D8', 'D9', 'E1', 'E10', 'E11', 'E12', 'E13', 'E14', 'E15', 'E16', 'E17', 'E18', 'E19', 'E2', 'E20', 'E3', 'E4', 'E5', 'E6', 'E7', 'E8', 'E9', 'I1', 'I2', 'I3', 'I4', 'I5', 'I6', 'I7', 'I8', 'I9', 'M1', 'M10', 'M11', 'M12', 'M13', 'M14', 'M15', 'M16', 'M17', 'M18', 'M2', 'M3', 'M4', 'M5', 'M6', 'M7', 'M8', 'M9', 'P1', 'P10', 'P11', 'P12', 'P13', 'P2', 'P3', 'P4', 'P5', 'P6', 'P7', 'P8', 'P9', 'S1', 'S10', 'S11', 'S12', 'S2', 'S3', 'S4', 'S5', 'S6', 'S7', 'S8', 'S9', 'V1', 'V10', 'V11', 'V12', 'V13', 'V2', 'V3', 'V4', 'V5', 'V6', 'V7', 'V8', 'V9', 'is_scored', 'target', 'lagged_risk_free_rate', 'lagged_market_forward_excess_returns'] shape: (10, 99)\n",
      "┌─────────┬─────┬─────┬─────┬───┬───────────┬───────────┬─────────────────────┬────────────────────┐\n",
      "│ date_id ┆ D1  ┆ D2  ┆ D3  ┆ … ┆ is_scored ┆ target    ┆ lagged_risk_free_ra ┆ lagged_market_forw │\n",
      "│ ---     ┆ --- ┆ --- ┆ --- ┆   ┆ ---       ┆ ---       ┆ te                  ┆ ard_excess_r…      │\n",
      "│ i64     ┆ f64 ┆ f64 ┆ f64 ┆   ┆ f64       ┆ f64       ┆ ---                 ┆ ---                │\n",
      "│         ┆     ┆     ┆     ┆   ┆           ┆           ┆ f64                 ┆ f64                │\n",
      "╞═════════╪═════╪═════╪═════╪═══╪═══════════╪═══════════╪═════════════════════╪════════════════════╡\n",
      "│ 8980    ┆ 0.0 ┆ 0.0 ┆ 0.0 ┆ … ┆ 1.0       ┆ 0.003541  ┆ 0.000161            ┆ 0.003068           │\n",
      "│ 8981    ┆ 0.0 ┆ 0.0 ┆ 0.0 ┆ … ┆ 1.0       ┆ -0.005964 ┆ 0.000162            ┆ -0.006437          │\n",
      "│ 8982    ┆ 0.0 ┆ 0.0 ┆ 0.0 ┆ … ┆ 1.0       ┆ -0.00741  ┆ 0.00016             ┆ -0.007882          │\n",
      "│ 8983    ┆ 0.0 ┆ 0.0 ┆ 0.0 ┆ … ┆ 1.0       ┆ 0.00542   ┆ 0.00016             ┆ 0.004949           │\n",
      "│ 8984    ┆ 0.0 ┆ 0.0 ┆ 0.0 ┆ … ┆ 1.0       ┆ 0.008357  ┆ 0.000159            ┆ 0.007887           │\n",
      "│ 8985    ┆ 0.0 ┆ 0.0 ┆ 0.0 ┆ … ┆ 1.0       ┆ -0.002896 ┆ 0.000159            ┆ -0.003365          │\n",
      "│ 8986    ┆ 0.0 ┆ 0.0 ┆ 0.0 ┆ … ┆ 1.0       ┆ 0.002457  ┆ 0.000155            ┆ 0.00199            │\n",
      "│ 8987    ┆ 0.0 ┆ 0.0 ┆ 1.0 ┆ … ┆ 1.0       ┆ 0.002312  ┆ 0.000156            ┆ 0.001845           │\n",
      "│ 8988    ┆ 0.0 ┆ 0.0 ┆ 0.0 ┆ … ┆ 1.0       ┆ 0.002891  ┆ 0.000156            ┆ 0.002424           │\n",
      "│ 8989    ┆ 0.0 ┆ 0.0 ┆ 0.0 ┆ … ┆ 0.0       ┆ 0.00831   ┆ 0.000156            ┆ 0.007843           │\n",
      "└─────────┴─────┴─────┴─────┴───┴───────────┴───────────┴─────────────────────┴────────────────────┘\n"
     ]
    }
   ],
   "source": [
    "train: pl.DataFrame = load_trainset()\n",
    "test: pl.DataFrame = load_testset() \n",
    "print(train.columns,train)\n",
    "print(test.columns,test)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "32bbbd50",
   "metadata": {
    "papermill": {
     "duration": 0.00369,
     "end_time": "2025-09-18T14:29:23.376685",
     "exception": false,
     "start_time": "2025-09-18T14:29:23.372995",
     "status": "completed"
    },
    "tags": []
   },
   "source": [
    "## Converting Return Prediction to Signal\n",
    "\n",
    "Here is an example of a potential function used to convert a prediction based on the market forward excess return to a daily signal position. "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "48a1c428",
   "metadata": {
    "papermill": {
     "duration": 0.004368,
     "end_time": "2025-09-18T14:29:23.912723",
     "exception": false,
     "start_time": "2025-09-18T14:29:23.908355",
     "status": "completed"
    },
    "tags": []
   },
   "source": [
    "## Generating the Train and Test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "fcc99460",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-09-18T14:29:23.922905Z",
     "iopub.status.busy": "2025-09-18T14:29:23.922582Z",
     "iopub.status.idle": "2025-09-18T14:29:24.036907Z",
     "shell.execute_reply": "2025-09-18T14:29:24.036134Z"
    },
    "papermill": {
     "duration": 0.121147,
     "end_time": "2025-09-18T14:29:24.038538",
     "exception": false,
     "start_time": "2025-09-18T14:29:23.917391",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\chenx\\AppData\\Local\\Temp\\ipykernel_5836\\3746280956.py:3: DeprecationWarning: `is_in` with a collection of the same datatype is ambiguous and deprecated.\n",
      "Please use `implode` to return to previous behavior.\n",
      "\n",
      "See https://github.com/pola-rs/polars/issues/22149 for more information.\n",
      "  train: pl.DataFrame = df.filter(pl.col('date_id').is_in(train.get_column('date_id')))\n",
      "C:\\Users\\chenx\\AppData\\Local\\Temp\\ipykernel_5836\\3746280956.py:4: DeprecationWarning: `is_in` with a collection of the same datatype is ambiguous and deprecated.\n",
      "Please use `implode` to return to previous behavior.\n",
      "\n",
      "See https://github.com/pola-rs/polars/issues/22149 for more information.\n",
      "  test: pl.DataFrame = df.filter(pl.col('date_id').is_in(test.get_column('date_id')))\n"
     ]
    }
   ],
   "source": [
    "df: pl.DataFrame = join_train_test_dataframes(train, test)\n",
    "df = df.fill_null(0)\n",
    "train: pl.DataFrame = df.filter(pl.col('date_id').is_in(train.get_column('date_id')))\n",
    "test: pl.DataFrame = df.filter(pl.col('date_id').is_in(test.get_column('date_id')))\n",
    "\n",
    "FEATURES: list[str] = [col for col in test.columns if col not in ['date_id', 'target']]\n",
    "\n",
    "dataset: DatasetOutput = split_dataset(train=train, test=test, features=FEATURES) \n",
    "\n",
    "X_train: pl.DataFrame = dataset.X_train\n",
    "X_test: pl.DataFrame = dataset.X_test\n",
    "y_train: pl.DataFrame = dataset.y_train\n",
    "y_test: pl.DataFrame = dataset.y_test\n",
    "scaler: StandardScaler = dataset.scaler "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "5034e5ee",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(9021, 96)\n"
     ]
    }
   ],
   "source": [
    "print(df.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c605fc63",
   "metadata": {
    "papermill": {
     "duration": 0.004102,
     "end_time": "2025-09-18T14:29:24.047139",
     "exception": false,
     "start_time": "2025-09-18T14:29:24.043037",
     "status": "completed"
    },
    "tags": []
   },
   "source": [
    "## Fitting the Model "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "d475d0a8",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-09-18T14:29:24.056837Z",
     "iopub.status.busy": "2025-09-18T14:29:24.056533Z",
     "iopub.status.idle": "2025-09-18T14:29:24.241199Z",
     "shell.execute_reply": "2025-09-18T14:29:24.240397Z"
    },
    "papermill": {
     "duration": 0.191239,
     "end_time": "2025-09-18T14:29:24.242695",
     "exception": false,
     "start_time": "2025-09-18T14:29:24.051456",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'enet_params' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[31m---------------------------------------------------------------------------\u001b[39m",
      "\u001b[31mNameError\u001b[39m                                 Traceback (most recent call last)",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[10]\u001b[39m\u001b[32m, line 2\u001b[39m\n\u001b[32m      1\u001b[39m model_cv: ElasticNetCV = ElasticNetCV(\n\u001b[32m----> \u001b[39m\u001b[32m2\u001b[39m     **asdict(\u001b[43menet_params\u001b[49m)\n\u001b[32m      3\u001b[39m )\n\u001b[32m      4\u001b[39m model_cv.fit(X_train, y_train) \n\u001b[32m      6\u001b[39m \u001b[38;5;66;03m# Fit the final model using the best alpha found by cross-validation\u001b[39;00m\n",
      "\u001b[31mNameError\u001b[39m: name 'enet_params' is not defined"
     ]
    }
   ],
   "source": [
    "model_cv: ElasticNetCV = ElasticNetCV(\n",
    "    **asdict(enet_params)\n",
    ")\n",
    "model_cv.fit(X_train, y_train) \n",
    "        \n",
    "# Fit the final model using the best alpha found by cross-validation\n",
    "model: ElasticNet = ElasticNet(alpha=model_cv.alpha_, l1_ratio=enet_params.l1_ratio) \n",
    "model.fit(X_train, y_train)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "df2ce087",
   "metadata": {
    "papermill": {
     "duration": 0.004199,
     "end_time": "2025-09-18T14:29:24.251659",
     "exception": false,
     "start_time": "2025-09-18T14:29:24.247460",
     "status": "completed"
    },
    "tags": []
   },
   "source": [
    "## Prediction Function via Kaggle Server"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "bcc9fa22",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-09-18T14:29:24.261930Z",
     "iopub.status.busy": "2025-09-18T14:29:24.261659Z",
     "iopub.status.idle": "2025-09-18T14:29:24.267826Z",
     "shell.execute_reply": "2025-09-18T14:29:24.266893Z"
    },
    "papermill": {
     "duration": 0.013274,
     "end_time": "2025-09-18T14:29:24.269463",
     "exception": false,
     "start_time": "2025-09-18T14:29:24.256189",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "def predict(test: pl.DataFrame) -> float:\n",
    "    # rename only if present, avoid raising when key missing\n",
    "    if 'lagged_forward_returns' in test.columns and 'target' not in test.columns:\n",
    "        test = test.rename({'lagged_forward_returns': 'target'}, strict=False)\n",
    "\n",
    "    if 'target' not in test.columns:\n",
    "        raise ValueError(\"Input test DataFrame must contain 'lagged_forward_returns' or 'target' column\")\n",
    "    \n",
    "    df: pl.DataFrame = test\n",
    "\n",
    "    # ensure all features are present after preprocessing\n",
    "    missing = [f for f in FEATURES if f not in df.columns]\n",
    "    if missing:\n",
    "        raise ValueError(f\"Missing feature columns after preprocessing: {missing}\")\n",
    "\n",
    "    X_test_df: pl.DataFrame = df.select(FEATURES)\n",
    "    X_test_np: np.ndarray = X_test_df.to_numpy()\n",
    "    X_test_scaled_np: np.ndarray = scaler.transform(X_test_np)\n",
    "    raw_pred: np.ndarray = model.predict(X_test_scaled_np)\n",
    "    return convert_ret_to_signal(raw_pred, ret_signal_params)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "b8cfe5ff",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[1.05144634 1.15416558 0.99950111 0.96067718 0.93712809 0.97670075\n",
      " 0.97337924 1.03456828 0.95085851 0.83524328 1.05144634 1.15416558\n",
      " 0.99950111 0.96067718 0.93712809 0.97670075 0.97337924 1.03456828\n",
      " 0.95085851 0.83524328]\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\chenx\\AppData\\Local\\Packages\\PythonSoftwareFoundation.Python.3.11_qbz5n2kfra8p0\\LocalCache\\local-packages\\Python311\\site-packages\\sklearn\\base.py:493: UserWarning: X does not have valid feature names, but StandardScaler was fitted with feature names\n",
      "  warnings.warn(\n",
      "C:\\Users\\chenx\\AppData\\Local\\Packages\\PythonSoftwareFoundation.Python.3.11_qbz5n2kfra8p0\\LocalCache\\local-packages\\Python311\\site-packages\\sklearn\\base.py:493: UserWarning: X does not have valid feature names, but ElasticNet was fitted with feature names\n",
      "  warnings.warn(\n"
     ]
    }
   ],
   "source": [
    "prediction = predict(test = test)\n",
    "print(prediction)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "015be7d2",
   "metadata": {
    "papermill": {
     "duration": 0.00438,
     "end_time": "2025-09-18T14:29:24.590374",
     "exception": false,
     "start_time": "2025-09-18T14:29:24.585994",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "MIN_INVESTMENT = 0\n",
    "MAX_INVESTMENT = 2\n",
    "\n",
    "\n",
    "class ParticipantVisibleError(Exception):\n",
    "    pass\n",
    "\n",
    "\n",
    "def score(solution: pd.DataFrame, submission: pd.DataFrame, row_id_column_name: str) -> float:\n",
    "    \"\"\"\n",
    "    Calculates a custom evaluation metric (volatility-adjusted Sharpe ratio).\n",
    "\n",
    "    This metric penalizes strategies that take on significantly more volatility\n",
    "    than the underlying market.\n",
    "\n",
    "    Returns:\n",
    "        float: The calculated adjusted Sharpe ratio.\n",
    "    \"\"\"\n",
    "\n",
    "    #if not pandas.api.types.is_numeric_dtype(submission['prediction']):\n",
    "    #    raise ParticipantVisibleError('Predictions must be numeric')\n",
    "\n",
    "    solution = solution\n",
    "    solution['position'] = submission['prediction']\n",
    "\n",
    "    if solution['position'].max() > MAX_INVESTMENT:\n",
    "        raise ParticipantVisibleError(f'Position of {solution[\"position\"].max()} exceeds maximum of {MAX_INVESTMENT}')\n",
    "    if solution['position'].min() < MIN_INVESTMENT:\n",
    "        raise ParticipantVisibleError(f'Position of {solution[\"position\"].min()} below minimum of {MIN_INVESTMENT}')\n",
    "\n",
    "    solution['strategy_returns'] = solution['risk_free_rate'] * (1 - solution['position']) + solution['position'] * solution['forward_returns']\n",
    "\n",
    "    # Calculate strategy's Sharpe ratio\n",
    "    strategy_excess_returns = solution['strategy_returns'] - solution['risk_free_rate']\n",
    "    strategy_excess_cumulative = (1 + strategy_excess_returns).prod()\n",
    "    strategy_mean_excess_return = (strategy_excess_cumulative) ** (1 / len(solution)) - 1\n",
    "    strategy_std = solution['strategy_returns'].std()\n",
    "\n",
    "    trading_days_per_yr = 252\n",
    "    if strategy_std == 0:\n",
    "        raise ParticipantVisibleError('Division by zero, strategy std is zero')\n",
    "    sharpe = strategy_mean_excess_return / strategy_std * np.sqrt(trading_days_per_yr)\n",
    "    strategy_volatility = float(strategy_std * np.sqrt(trading_days_per_yr) * 100)\n",
    "\n",
    "    # Calculate market return and volatility\n",
    "    market_excess_returns = solution['forward_returns'] - solution['risk_free_rate']\n",
    "    market_excess_cumulative = (1 + market_excess_returns).prod()\n",
    "    market_mean_excess_return = (market_excess_cumulative) ** (1 / len(solution)) - 1\n",
    "    market_std = solution['forward_returns'].std()\n",
    "\n",
    "    market_volatility = float(market_std * np.sqrt(trading_days_per_yr) * 100)\n",
    "\n",
    "    if market_volatility == 0:\n",
    "        raise ParticipantVisibleError('Division by zero, market std is zero')\n",
    "\n",
    "    # Calculate the volatility penalty\n",
    "    excess_vol = max(0, strategy_volatility / market_volatility - 1.2) if market_volatility > 0 else 0\n",
    "    vol_penalty = 1 + excess_vol\n",
    "\n",
    "    # Calculate the return penalty\n",
    "    return_gap = max(\n",
    "        0,\n",
    "        (market_mean_excess_return - strategy_mean_excess_return) * 100 * trading_days_per_yr,\n",
    "    )\n",
    "    return_penalty = 1 + (return_gap**2) / 100\n",
    "\n",
    "    # Adjust the Sharpe ratio by the volatility and return penalty\n",
    "    adjusted_sharpe = sharpe / (vol_penalty * return_penalty)\n",
    "    return min(float(adjusted_sharpe), 1_000_000)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "c769e26b",
   "metadata": {},
   "outputs": [
    {
     "ename": "IndexError",
     "evalue": "only integers, slices (`:`), ellipsis (`...`), numpy.newaxis (`None`) and integer or boolean arrays are valid indices",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mIndexError\u001b[0m                                Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[18], line 1\u001b[0m\n\u001b[1;32m----> 1\u001b[0m \u001b[43mscore\u001b[49m\u001b[43m(\u001b[49m\u001b[43mtest\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mprediction\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[38;5;124;43mdate_id\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[43m)\u001b[49m\n",
      "Cell \u001b[1;32mIn[17], line 24\u001b[0m, in \u001b[0;36mscore\u001b[1;34m(solution, submission, row_id_column_name)\u001b[0m\n\u001b[0;32m     20\u001b[0m \u001b[38;5;66;03m#if not pandas.api.types.is_numeric_dtype(submission['prediction']):\u001b[39;00m\n\u001b[0;32m     21\u001b[0m \u001b[38;5;66;03m#    raise ParticipantVisibleError('Predictions must be numeric')\u001b[39;00m\n\u001b[0;32m     23\u001b[0m solution \u001b[38;5;241m=\u001b[39m solution\n\u001b[1;32m---> 24\u001b[0m solution[\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mposition\u001b[39m\u001b[38;5;124m'\u001b[39m] \u001b[38;5;241m=\u001b[39m \u001b[43msubmission\u001b[49m\u001b[43m[\u001b[49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[38;5;124;43mprediction\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[43m]\u001b[49m\n\u001b[0;32m     26\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m solution[\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mposition\u001b[39m\u001b[38;5;124m'\u001b[39m]\u001b[38;5;241m.\u001b[39mmax() \u001b[38;5;241m>\u001b[39m MAX_INVESTMENT:\n\u001b[0;32m     27\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m ParticipantVisibleError(\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mPosition of \u001b[39m\u001b[38;5;132;01m{\u001b[39;00msolution[\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mposition\u001b[39m\u001b[38;5;124m\"\u001b[39m]\u001b[38;5;241m.\u001b[39mmax()\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m exceeds maximum of \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mMAX_INVESTMENT\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m'\u001b[39m)\n",
      "\u001b[1;31mIndexError\u001b[0m: only integers, slices (`:`), ellipsis (`...`), numpy.newaxis (`None`) and integer or boolean arrays are valid indices"
     ]
    }
   ],
   "source": [
    "score(test, prediction, 'date_id')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d72d9ef2",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ============================================================================\n",
    "# Supervised Learning Program with Score Function as Reward\n",
    "# ============================================================================\n",
    "\n",
    "from sklearn.ensemble import RandomForestRegressor, GradientBoostingRegressor\n",
    "from sklearn.linear_model import Ridge, Lasso\n",
    "from sklearn.model_selection import cross_validate\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')\n",
    "\n",
    "class SupervisedLearningReward:\n",
    "    \"\"\"\n",
    "    A supervised learning framework that uses the score function as the reward metric.\n",
    "    \"\"\"\n",
    "    \n",
    "    def __init__(self, train_data: pl.DataFrame, test_data: pl.DataFrame, features: list[str]):\n",
    "        \"\"\"\n",
    "        Initialize the supervised learning framework.\n",
    "        \n",
    "        Args:\n",
    "            train_data: Training DataFrame with features and target\n",
    "            test_data: Test DataFrame with features and target\n",
    "            features: List of feature column names\n",
    "        \"\"\"\n",
    "        self.train_data = train_data\n",
    "        self.test_data = test_data\n",
    "        self.features = features\n",
    "        self.models_results = []\n",
    "        \n",
    "    def prepare_data_for_scoring(self, predictions: pd.Series, test_data: pl.DataFrame) -> tuple:\n",
    "        \"\"\"\n",
    "        Prepare solution and submission DataFrames required by the score function.\n",
    "        \n",
    "        Args:\n",
    "            predictions: Model predictions (positions to take)\n",
    "            test_data: Test DataFrame containing market data\n",
    "            \n",
    "        Returns:\n",
    "            Tuple of (solution_df, submission_df)\n",
    "        \"\"\"\n",
    "        solution = test_data.select(['date_id', 'risk_free_rate', 'forward_returns']).to_pandas()\n",
    "        submission = pd.DataFrame({'prediction': predictions})\n",
    "        \n",
    "        return solution, submission\n",
    "    \n",
    "    def evaluate_model(self, model, X_train, y_train, X_test, y_test, test_data: pl.DataFrame, model_name: str) -> dict:\n",
    "        \"\"\"\n",
    "        Train a model and evaluate it using the score function.\n",
    "        \n",
    "        Args:\n",
    "            model: Scikit-learn model instance\n",
    "            X_train, y_train: Training features and target\n",
    "            X_test, y_test: Test features and target\n",
    "            test_data: Test data for scoring\n",
    "            model_name: Name of the model for tracking\n",
    "            \n",
    "        Returns:\n",
    "            Dictionary with evaluation results including score\n",
    "        \"\"\"\n",
    "        # Train the model\n",
    "        model.fit(X_train, y_train)\n",
    "        \n",
    "        # Make predictions\n",
    "        train_predictions = model.predict(X_train)\n",
    "        test_predictions = model.predict(X_test)\n",
    "        \n",
    "        # Clip predictions to valid range [MIN_INVESTMENT, MAX_INVESTMENT]\n",
    "        test_predictions = np.clip(test_predictions, MIN_INVESTMENT, MAX_INVESTMENT)\n",
    "        \n",
    "        # Calculate score using the reward function\n",
    "        solution, submission = self.prepare_data_for_scoring(test_predictions, test_data)\n",
    "        \n",
    "        try:\n",
    "            reward_score = score(solution, submission, 'date_id')\n",
    "        except ParticipantVisibleError as e:\n",
    "            reward_score = -float('inf')\n",
    "            print(f\"  Error: {e}\")\n",
    "        \n",
    "        # Calculate traditional metrics\n",
    "        from sklearn.metrics import mean_squared_error, r2_score, mean_absolute_error\n",
    "        mse = mean_squared_error(y_test, test_predictions)\n",
    "        mae = mean_absolute_error(y_test, test_predictions)\n",
    "        r2 = r2_score(y_test, test_predictions)\n",
    "        \n",
    "        result = {\n",
    "            'model_name': model_name,\n",
    "            'reward_score': reward_score,\n",
    "            'mse': mse,\n",
    "            'mae': mae,\n",
    "            'r2': r2,\n",
    "            'model': model,\n",
    "            'predictions': test_predictions\n",
    "        }\n",
    "        \n",
    "        self.models_results.append(result)\n",
    "        return result\n",
    "    \n",
    "    def train_and_compare(self):\n",
    "        \"\"\"\n",
    "        Train multiple models and compare them using the score function.\n",
    "        \"\"\"\n",
    "        # Convert polars to numpy for scikit-learn\n",
    "        X_train = X_train.to_numpy()\n",
    "        y_train = y_train.to_numpy()\n",
    "        X_test = X_test.to_numpy()\n",
    "        y_test = y_test.to_numpy()\n",
    "        \n",
    "        models = {\n",
    "            'Linear Regression': Ridge(alpha=1.0),\n",
    "            'Lasso': Lasso(alpha=0.01, max_iter=5000),\n",
    "            'Random Forest': RandomForestRegressor(n_estimators=100, random_state=42, n_jobs=-1),\n",
    "            'Gradient Boosting': GradientBoostingRegressor(n_estimators=100, random_state=42),\n",
    "        }\n",
    "        \n",
    "        print(\"Training models and evaluating with score function...\\n\")\n",
    "        \n",
    "        for model_name, model in models.items():\n",
    "            print(f\"Training {model_name}...\")\n",
    "            result = self.evaluate_model(\n",
    "                model, X_train, y_train, X_test, y_test, \n",
    "                test_data, model_name\n",
    "            )\n",
    "            print(f\"  Reward Score: {result['reward_score']:.4f}\")\n",
    "            print(f\"  MSE: {result['mse']:.6f}, R²: {result['r2']:.4f}\\n\")\n",
    "        \n",
    "        return self.get_best_model()\n",
    "    \n",
    "    def get_best_model(self):\n",
    "        \"\"\"Get the model with the highest score.\"\"\"\n",
    "        if not self.models_results:\n",
    "            return None\n",
    "        \n",
    "        best = max(self.models_results, key=lambda x: x['reward_score'])\n",
    "        print(f\"Best Model: {best['model_name']} with Score: {best['reward_score']:.4f}\")\n",
    "        return best\n",
    "\n",
    "\n",
    "# ============================================================================\n",
    "# Run the Supervised Learning Framework\n",
    "# ============================================================================\n",
    "\n",
    "print(\"=\" * 70)\n",
    "print(\"SUPERVISED LEARNING WITH SCORE-BASED REWARD\")\n",
    "print(\"=\" * 70)\n",
    "\n",
    "learner = SupervisedLearningReward(train, test, FEATURES)\n",
    "best_result = learner.train_and_compare()\n",
    "\n",
    "print(\"\\n\" + \"=\" * 70)\n",
    "print(\"Summary of All Models:\")\n",
    "print(\"=\" * 70)\n",
    "for result in learner.models_results:\n",
    "    print(f\"{result['model_name']:25} | Score: {result['reward_score']:10.4f} | R²: {result['r2']:8.4f}\")"
   ]
  }
 ],
 "metadata": {
  "kaggle": {
   "accelerator": "none",
   "dataSources": [
    {
     "databundleVersionId": 13750964,
     "sourceId": 111543,
     "sourceType": "competition"
    }
   ],
   "dockerImageVersionId": 31089,
   "isGpuEnabled": false,
   "isInternetEnabled": false,
   "language": "python",
   "sourceType": "notebook"
  },
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.2"
  },
  "papermill": {
   "default_parameters": {},
   "duration": 12.306793,
   "end_time": "2025-09-18T14:29:25.314159",
   "environment_variables": {},
   "exception": null,
   "input_path": "__notebook__.ipynb",
   "output_path": "__notebook__.ipynb",
   "parameters": {},
   "start_time": "2025-09-18T14:29:13.007366",
   "version": "2.6.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
